{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8587acde",
   "metadata": {},
   "source": [
    "# Observation and Action\n",
    "\n",
    "<img align=\"right\" src=\"figs/observation_demo.png\" width=260>\n",
    "\n",
    "MetaDrive provides various kinds of sensory input, as illustrated in the next figure.\n",
    "For low-level sensors, RGB cameras, depth cameras, semantic camera, instance camera and Lidar can be placed anywhere in the scene with adjustable\n",
    "parameters such as view field and the laser number.\n",
    "Meanwhile, the high-level scene information including the road information and nearby vehicles' information like velocity and heading can also be provided as the observation.\n",
    "\n",
    "Note that MetaDrive aims at providing an efficient platform to benchmark RL research,\n",
    "therefore we improve the simulation efficiency at the cost of photorealistic rendering effect.\n",
    "\n",
    "In this page, we describe the optional observation forms in current MetaDrive version and discuss how to implement new forms of observation subject to your own tasks.\n",
    "\n",
    "\n",
    "## Observations\n",
    "There are three kinds of observations we usually used for training agents:\n",
    "- LidarStateObservation\n",
    "- ImageStateObservation\n",
    "- TopDownObservation\n",
    "\n",
    "### LidarStateObservation\n",
    "MetaDrive provides a state vector containing necessary information to navigation tasks.\n",
    "We use this state vector in almost all existing RL experiments such as the Generalization, MARL and Safe RL experiments.\n",
    "The state vector consist of three parts:\n",
    "1. **Ego State**: current states such as the steering, heading, velocity and relative distance to boundaries, implemented in the `vehicle_state` function of [StateObservation](https://github.com/metadriverse/metadrive/blob/main/metadrive/obs/state_obs.py#L9). Please find the detailed meaning of each state dimension in the code.\n",
    "2. **Navigation**: the navigation information that guides the vehicle toward the destination. Concretely, MetaDrive first computes the route from the spawn point to the destination of the ego vehicle. Then a set of checkpoints are scattered across the whole route with certain intervals. The relative distance and direction to the next checkpoint and the next next checkpoint will be given as the navigation information. This part is implemented in the `_get_info_for_checkpoint` function of [Navigation Class](https://github.com/metadriverse/metadrive/blob/9a89962e72c709e60d4a5bc19ce5f27d96027401/metadrive/component/vehicle_navigation_module/base_navigation.py#L13C10-L13C10).\n",
    "3. **Surrounding**: the surrounding information is encoded by a vector containing the Lidar-like cloud points. The data is generated by the [Lidar Class](https://github.com/metadriverse/metadrive/blob/main/metadrive/component/vehicle_module/lidar.py#L16). We typically use 240 lasers (single-agent) and 70 lasers (multi-agent) to scan the neighboring area with radius 50 meters.\n",
    "\n",
    "The above information is normalized to [0,1] and concatenated into a state vector by the [LidarStateObservation Class](https://github.com/metadriverse/metadrive/blob/main/metadrive/envs/observation_type.py) and fed to the RL agents.\n",
    "\n",
    "\n",
    "### ImageStateObservation\n",
    "\n",
    "\n",
    ".. image:: figs/rgb_obs.png\n",
    "   :width: 350\n",
    "   :align: center\n",
    "\n",
    ".. image:: figs/depth_obs.jpg\n",
    "   :width: 350\n",
    "   :align: center\n",
    "\n",
    "\n",
    "MetaDrive supports visuomotor tasks by turning on the rendering during the training.\n",
    "The above figure shows the images captured by RGB camera (left) and depth camera (right).\n",
    "In this section, we discuss how to utilize such observation in a **headless** machine, such as computing node in cluster\n",
    "or other remote server.\n",
    "Before using such function in your project, please make sure the offscreen rendering is working in your\n",
    "machine. The setup tutorial is at :ref:`install_headless`.\n",
    "\n",
    "Now we can setup the vision-based observation in MetaDrive:\n",
    "\n",
    "* Step 1. Set the `config[\"image_observation\"] = True` to tell MetaDrive maintaining a image buffer in memory even no popup window exists.\n",
    "* Step 2. Set the `config[\"vehicle_config\"][\"image_source\"]` to `\"rgb_camera\"` or `\"depth_camera\"` according to your demand.\n",
    "* Step 3. The image size (width and height) will be determined by the camera parameters. The default setting is (84, 84) following the image size in Atari. You can customize the size by configuring `config[\"vehicle_config\"][\"rgb_camera\"]`. For example, `config[\"vehicle_config\"][\"rgb_camera\"] = (200, 88)` means that the image has 200 pixels in width and 88 pixels in height.\n",
    "\n",
    "There is a demo script using RGB camera as observation::\n",
    "\n",
    "    python -m metadrive.examples.drive_in_single_agent_env --observation rgb_camera\n",
    "\n",
    "The script should print a message:\n",
    "\n",
    ".. code-block:: text\n",
    "\n",
    "    The observation is a dict with numpy arrays as values:  {'image': (84, 84, 3), 'state': (21,)}\n",
    "\n",
    "The image rendering consumes memory in the first GPU of your machine (if any). Please be careful when using this.\n",
    "\n",
    "\n",
    "If you feel the visual data collection is slow, why not try our advanced offscreen render: :ref:`install_render_cuda`.\n",
    "After verifying your installation, set `config[\"image_on_cuda\"] = True` to get **10x** faster data collection!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5bce5",
   "metadata": {},
   "source": [
    "### TopDownObservation\n",
    "<img align=\"center\" src=\"figs/top_down_obs.png\" width=600>\n",
    "\n",
    "MetaDrive also supports Top-down semantic maps. We provide a handy example to illustrate the utilization of Top-down observation in [top_down_metadrive.py]( https://github.com/metadriverse/metadrive/blob/main/metadrive/examples/top_down_metadrive.py).\n",
    "You can enjoy this demo via\n",
    "```bash\n",
    "python -m metadrive.examples.top_down_metadrive\n",
    "```\n",
    "The following is a minimal script to use Top-down observation.\n",
    "The `TopDownMetaDrive` is a wrapper class on `MetaDriveEnv` which overrides observation to pygame top-down renderer.\n",
    "The native observation of this setting is a numpy array with shape `[84, 84, 5]` and all entries fall into [0, 1].\n",
    "The above figure shows the semantic meaning of each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadrive import TopDownMetaDrive\n",
    "\n",
    "env = TopDownMetaDrive()\n",
    "try:\n",
    "    o,i = env.reset()\n",
    "    for s in range(1, 100000):\n",
    "        o, r, tm, tc, info = env.step([0, 1])\n",
    "        env.render(mode=\"top_down\")\n",
    "        if tm or tc:\n",
    "            break\n",
    "            env.reset()\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be60818",
   "metadata": {},
   "source": [
    "## Action\n",
    "\n",
    "\n",
    "MetaDrive receives normalized action as input to control each target vehicle: :math:`\\mathbf a = [a_1, a_2]^T \\in [-1, 1]^2`.\n",
    "\n",
    "At each environmental time step, MetaDrive converts the normalized action into the steering :math:`u_s` (degree), acceleration :math:`u_a` (hp) and brake signal :math:`u_b` (hp) in the following ways:\n",
    "\n",
    "\n",
    ".. math::\n",
    "\n",
    "    u_s & = S_{max} a_1 ~\\\\\n",
    "    u_a & = F_{max} \\max(0, a_2) ~\\\\\n",
    "    u_b & = -B_{max} \\min(0, a_2)\n",
    "\n",
    "wherein :math:`S_{max}` (degree)  is the maximal steering angle, :math:`F_{max}` (hp) is the maximal engine force, and :math:`B_{max}` (hp) is the maximal brake force.\n",
    "Since the accurate values of these parameters are varying across different types of vehicle, please refer to the `VehicleParameterSpace Class <https://github.com/metadriverse/metadrive/blob/main/metadrive/utils/space.py#L219) for details.\n",
    "\n",
    "By such design, the action space for each agent is always fixed to `gym.spaces.Box(low=-1.0, high=1.0, shape=(2, ))`. However, we provides a config named `extra_action_dim` (int) which allows user to add more dimensions in the action space.\n",
    "For example, if we set `config[\"extra_action_dim\"] = 1`, then the action space for each agent will become `Box(-1.0, 1.0, shape=(3, ))`. This allow the user to write environment wrapper that introduce more input action dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
