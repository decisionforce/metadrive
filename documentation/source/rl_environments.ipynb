{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b088aca",
   "metadata": {},
   "source": [
    "# RL Environments\n",
    "\n",
    "By default, there are 4 off-the-shelf RL environments:\n",
    "- Generalization environment\n",
    "- Safe RL environment\n",
    "- MARL environment\n",
    "- Real-world environment\n",
    "\n",
    "\n",
    "## Generalization Environment\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hL0XDfNHYjA?si=7cn1CpzgpNAf8OAd\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "We developed an RL environment through procedural generation where maps are composed by connecting various types of blocks and then traffic vehicles are scattered on maps randomly.\n",
    "Thus the environment can generate an unlimited number of diverse driving scenarios.\n",
    "By training RL agents in one set of scenarios and testing them in another held-out set, we can benchmark the generalizability of the driving policy. \n",
    "\n",
    "\n",
    "<img src=\"figs/blocks_and_big_case_page.jpg\" width=\"600\" class=\"center\">\n",
    "\n",
    "\n",
    "The following script creates a basic environment that can be used for such purpose:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadrive import MetaDriveEnv\n",
    "import tqdm\n",
    "\n",
    "training_env = MetaDriveEnv(dict(\n",
    "    num_scenarios=1000,\n",
    "    start_seed=1000,\n",
    "    random_lane_width=True,\n",
    "    random_agent_model=True,\n",
    "    random_lane_num=True\n",
    "))\n",
    "\n",
    "\n",
    "test_env = MetaDriveEnv(dict(\n",
    "    num_scenarios=200,\n",
    "    start_seed=0,\n",
    "    random_lane_width=True,\n",
    "    random_agent_model=True,\n",
    "    random_lane_num=True\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20484c04",
   "metadata": {},
   "source": [
    "User can specify the training set with 1000 driving scenarios by setting `num_scenarios=1000` and `start_seed=1000`, while creating the test set by setting `num_scenarios=200` and `start_seed=0`.\n",
    "In this case, the scenarios generated by random seeds [1000, 1999] will be used to train the agents and those by [0, 199] will be used to test the trained agent.\n",
    "\n",
    "**Note: Please note that each process should only have one single MetaDrive instance due to the limit of the underlying simulation engine, but sometimes we want to have one environment for training and the other one for testing.** There are two ways to overcome this:\n",
    "1. Launching the training environment and test environment in two separate processes using tools like `Ray` or `stablebaseline3.SubprocVecEnv`. Generally, We use [Ray/RLLib](https://docs.ray.io/en/latest/rllib.html) to train RL agents. The training and test environments are naturally hosted in training workers (processes) and evaluation workers. Therefore we do not worry about this singleton problem.\n",
    "2. Closing the training environment `training_env.close()` before launch the test environment via `test_env.reset()`. After evaluation, it is allowed to restore the training environment after closing the test environment by simply `training_env.reset()`. An example is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fcbe8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 1000, Num Scenarios : 1000\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training env, start_seed: 1000, end_seed: 2000\n",
      "test env, start_seed: 0, end_seed: 200\n",
      "\n",
      "Start fake training epoch 0...\n",
      "Evaluate checkpoint for training epoch 0...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 1000, Num Scenarios : 1000\u001b[0m\n",
      "\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n",
      "\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n",
      "\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start fake training epoch 1...\n",
      "Evaluate checkpoint for training epoch 1...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_env_info(env, name):\n",
    "    print(\"{} env, start_seed: {}, end_seed: {}\".format(name, env.start_seed, env.start_seed+env.num_scenarios))\n",
    "\n",
    "print_env_info(training_env, \"training\")\n",
    "print_env_info(test_env, \"test\")\n",
    "\n",
    "for training_epoch in range(2):\n",
    "    # training\n",
    "    training_env.reset()\n",
    "    print(\"\\nStart fake training epoch {}...\".format(training_epoch))\n",
    "    for _ in range(10):\n",
    "        # execute 10 step\n",
    "        training_env.step(training_env.action_space.sample())\n",
    "    training_env.close()\n",
    "\n",
    "    # evaluation\n",
    "    print(\"Evaluate checkpoint for training epoch {}...\\n\".format(training_epoch))\n",
    "    test_env.reset()\n",
    "    for _ in range(10):\n",
    "        # execute 10 evaluation step\n",
    "        test_env.step(test_env.action_space.sample())\n",
    "    test_env.close()\n",
    "\n",
    "assert test_env.config is not training_env.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75e6b1",
   "metadata": {},
   "source": [
    "The other config `dict(random_lane_width=True, random_agent_model=True, random_lane_num=True)` specifies that the agent model, lane num and lane width will be randomized to make the scenarios more diverse. In the following example, we sample 50 scenarios from the training set and show the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4764802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadrive.component.vehicle.vehicle_type import vehicle_type\n",
    "\n",
    "env_seed=1000\n",
    "lane_nums = set()\n",
    "lane_widths = set()\n",
    "vehicle_models = set()\n",
    "traffic_vehicle_models = set()\n",
    "\n",
    "maps_to_sample = 50\n",
    "end_seed = training_env.config[\"start_seed\"] + maps_to_sample\n",
    "for env_seed in tqdm.tqdm(range(training_env.config[\"start_seed\"], end_seed)):\n",
    "    \n",
    "    # use `seed` argument to choose which scenario to run\n",
    "    training_env.reset(seed=env_seed)\n",
    "    \n",
    "    # collect statistics\n",
    "    lane_nums.add(training_env.current_map.config[\"lane_num\"]) \n",
    "    lane_widths.add(training_env.current_map.config[\"lane_width\"])\n",
    "    vehicle_models.add(training_env.vehicle.__class__.__name__)\n",
    "    traffic_models = set([obj.__class__ for obj in training_env.engine.traffic_manager.spawned_objects.values()])\n",
    "    traffic_vehicle_models = traffic_vehicle_models.union(traffic_models)\n",
    "    assert vehicle_type[training_env.vehicle.config[\"vehicle_model\"]] is training_env.vehicle.__class__\n",
    "    \n",
    "training_env.close()\n",
    "\n",
    "print(\"Number of lanes in {} maps are: {}\".format(maps_to_sample, lane_nums))\n",
    "print(\"{} maps have {} different widths\".format(maps_to_sample, len(lane_widths)))\n",
    "print(\"The policy is learning to drive {} types of vehicles\".format(len(vehicle_models)))\n",
    "print(\"There are {} types of traffic vehicles\".format(len(traffic_vehicle_models)))\n",
    "\n",
    "\n",
    "assert lane_nums == {2, 3}\n",
    "assert len(lane_widths) == 50\n",
    "assert len(vehicle_models) == 5\n",
    "assert len(traffic_vehicle_models) == len(vehicle_models) - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb37d09",
   "metadata": {},
   "source": [
    "Actually, we provide an upgraded version for generalization environment with full PG functionality and can additionally randomize the dynamics of ego vehicle.\n",
    "The environment is called `VaryingDynamicsEnv` and you can control the `random_dynamics` dict in the config\n",
    "to adjust the randomizing range of specific dynamics parameters.\n",
    "In the below example (which is also the default config), we randomize the dynamics of vehicle to the lowest and highest limit we recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadrive.envs.varying_dynamics_env import VaryingDynamicsEnv\n",
    "from metadrive.component.vehicle.vehicle_type import vehicle_type\n",
    "import tqdm\n",
    "\n",
    "training_env = VaryingDynamicsEnv(dict(\n",
    "        num_scenarios=1000,  \n",
    "        \n",
    "        # Stop randomizing them\n",
    "        # random_lane_width=True,\n",
    "        # random_agent_model=True,\n",
    "        # random_lane_num=True\n",
    "    \n",
    "        # We will sample each parameter from (min_value, max_value)\n",
    "        # You can set it to None to stop randomizing the parameter.\n",
    "        random_dynamics=dict(\n",
    "            max_engine_force=(100, 3000),\n",
    "            max_brake_force=(20, 600),\n",
    "            wheel_friction=(0.1, 2.5),\n",
    "            max_steering=(10, 80),  # The maximum steering angle if action = +-1\n",
    "            mass=(300, 3000)\n",
    "        )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_seed=1000\n",
    "lane_nums = set()\n",
    "lane_widths = set()\n",
    "vehicle_models = set()\n",
    "traffic_vehicle_models = set()\n",
    "\n",
    "# collect more\n",
    "to_collect = [\"max_engine_force\", \"max_brake_force\", \"wheel_friction\", \"max_steering\", \"mass\"]\n",
    "to_collect_set = {k: set() for k in to_collect}\n",
    "\n",
    "maps_to_sample = 50\n",
    "end_seed = training_env.config[\"start_seed\"] + maps_to_sample\n",
    "for env_seed in tqdm.tqdm(range(training_env.config[\"start_seed\"], end_seed)):\n",
    "    \n",
    "    # use `seed` argument to choose which scenario to run\n",
    "    training_env.reset(seed=env_seed)\n",
    "    \n",
    "    # collect statistics\n",
    "    lane_nums.add(training_env.current_map.config[\"lane_num\"]) \n",
    "    lane_widths.add(training_env.current_map.config[\"lane_width\"])\n",
    "    vehicle_models.add(training_env.vehicle.__class__.__name__)\n",
    "    traffic_models = set([obj.__class__ for obj in training_env.engine.traffic_manager.spawned_objects.values()])\n",
    "    traffic_vehicle_models = traffic_vehicle_models.union(traffic_models)\n",
    "    assert vehicle_type[training_env.vehicle.config[\"vehicle_model\"]] is training_env.vehicle.__class__\n",
    "    \n",
    "    # collect more\n",
    "    for k, v in to_collect_set.items():\n",
    "        v.add(training_env.vehicle.config[k])\n",
    "    \n",
    "training_env.close()\n",
    "\n",
    "print(\"Number of lanes in {} maps are: {}\".format(maps_to_sample, lane_nums))\n",
    "print(\"{} maps have {} different widths\".format(maps_to_sample, len(lane_widths)))\n",
    "print(\"The policy is learning to drive vehicles with {} different dyamics\".format(len(to_collect_set[\"wheel_friction\"])))\n",
    "\n",
    "assert all([len(s)==50 for s in to_collect_set.values()])\n",
    "assert lane_nums == {3}\n",
    "assert len(lane_widths) == 1\n",
    "assert vehicle_models == set([vehicle_type[\"varying_dynamics\"].__name__])\n",
    "assert len(traffic_vehicle_models) == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97cf99",
   "metadata": {},
   "source": [
    "In the very early stage of MetaDrive, we have experimented randomizing the `wheel_friction` in the training environment.\n",
    "We find that `wheel_friction > 1.2` makes little impact to the performance. So you can try a training environment\n",
    "with `wheel_friction in [1.0, 1.4)` and test the trained agent in `wheel_friction in [0.6, 1.0)`.\n",
    "The training environment is significantly easier than the test environment.\n",
    "We are expecting that the agent trained in less training scenarios will perform poorly in the test environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2f93f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------\n",
    "\n",
    "<img src=\"figs/metadrive-envs.jpg\" width=\"600\" class=\"center\">\n",
    "\n",
    "\n",
    "## Safety Environments\n",
    "\n",
    "\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6YNgwxEvYtg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "\n",
    "Safety is a major concern for the trial-and-error nature of RL.\n",
    "As driving itself is a safety-critical application, it is essential to evaluate the constrained optimization methods under the domain of autonomous driving.\n",
    "We therefore define a new suite of environments to benchmark the **safe exploration** in RL.\n",
    "\n",
    "\n",
    "As shown in the left panel of the figure above, we randomly display static and movable obstacles in the traffic.\n",
    "Different from the generalization task, we do not terminate the agent if a collision with those obstacles and traffic vehicles happens.\n",
    "Instead, we allow agent to continue driving but flag the crash with a cost +1.\n",
    "Thus as safe exploration task, the learning agent is required to balance the reward and the cost to solve the constrained optimization problem.\n",
    "\n",
    "\n",
    "The following script can setup such environment. Same as in generalization environment, you can also specify the number of environment and the start seed to initialize two sets of environments to train and test the RL agents and benchmark their safety generalization. The environment-specific parameter is `accident_prob`, which controls the density of obstacles on the road. Apart from this, all parameters are the same as generalization environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadrive import SafeMetaDriveEnv\n",
    "\n",
    "env=SafeMetaDriveEnv(dict(\n",
    "    num_scenarios=1000,\n",
    "    start_seed=0,\n",
    "    accident_prob = 0.8, # accepted parameter is in [0, 1.0]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649a886",
   "metadata": {},
   "source": [
    "You can also experience the safety environment via \n",
    "\n",
    "```bash\n",
    "python -m metadrive.examples.drive_in_safe_metadrive_env\n",
    "```\n",
    "\n",
    "## Multi-agent Environments\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1-sXZv2ZzXM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "\n",
    "As shown in the above figure,\n",
    "we develop a set of environments to evaluate MARL methods for simulating traffic flow.\n",
    "The descriptions and typical settings of the six traffic environments are as follows:\n",
    "\n",
    "1. **Roundabout**: A four-way roundabout with two lanes. 40 vehicles spawn during environment reset. This environment includes merge and split junctions.\n",
    "2. **Intersection**: An unprotected four-way intersection allowing bi-directional traffic as well as U-turns. Negotiation and social behaviors are expected to solve this environment. We initialize 30 vehicles.\n",
    "3. **Tollgate**: Tollgate includes narrow roads to spawn agents and ample space in the middle with multiple tollgates. The tollgates create static obstacles where the crashing is prohibited. We force agent to stop at the middle of tollgate for 3s. The agent will fail if they exit the tollgate before being allowed to pass. 40 vehicles are initialized. Complex behaviors such as deceleration and queuing are expected. Additional states such as whether vehicle is in tollgate and whether the tollgate is blocked are given.\n",
    "4. **Bottleneck**: Complementary to Tollgate, Bottleneck contains a narrow bottleneck lane in the middle that forces the vehicles to yield to others. We initialize 20 vehicles.\n",
    "5. **Parking Lot**: A compact environment with 8 parking slots. Spawn points are scattered in both parking lots or in external roads. 10 vehicles spawn initially and need to navigate toward external roads or enter parking lots. In this environment, we allow agents to back their cars to spare space for others.  Maneuvering and yielding are the key to solve this task.\n",
    "6. **PGMA** (Procedural Generation Multi-Agent environment): We reuse the procedurally generated scenarios in the generalization environment and replaces the traffic vehicles by controllable target vehicles. These environments contain rich interactions between agents and complex road structures. This multi-agent environment introduces new challenge under the setting of mixed motive RL. Each constituent agent in this traffic system is self-interested and the relationship between agents is constantly changing.\n",
    "\n",
    "In Multi-agent environment, the termination criterion for each vehicle is identical to that in single-agent environment.\n",
    "We explicitly add two config to adjust the termination processing in MARL: `crash_done = True` and `out_of_road_done = True`.\n",
    "They denotes whether to terminate the agent episode if crash / out of road happens.\n",
    "\n",
    "Besides, in Multi-agent environment, the controllable target vehicles consistently respawn in the scene if old target vehicles are terminated.\n",
    "To limit the length of *environmental episode*, we also introduce a config `horizon = 1000` in MARL environments.\n",
    "The environmental episode has a **minimal length** of `horizon` steps and the environment will stop spawning new target vehicles if this horizon is exceeded.\n",
    "If you wish to disable the respawning mechanism in MARL, set the config `allow_respawn = False`. In this case, the environmental episode will terminate if no active vehicles are in the scene.\n",
    "\n",
    "\n",
    "You can try to drive a vehicle in Multi-agent environment through this example:\n",
    "```bash\n",
    "# Options for --env: roundabout, intersection, tollgate, bottleneck, parkinglot, pgma\n",
    "python -m metadrive.examples.drive_in_multi_agent_env --env pgma\n",
    "```\n",
    "\n",
    "The following script initialize arbitrary Multi-agent environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadrive import (\n",
    "    MultiAgentMetaDrive,\n",
    "    MultiAgentTollgateEnv,\n",
    "    MultiAgentBottleneckEnv,\n",
    "    MultiAgentIntersectionEnv,\n",
    "    MultiAgentRoundaboutEnv,\n",
    "    MultiAgentParkingLotEnv\n",
    ")\n",
    "\n",
    "envs_classes = dict(\n",
    "    roundabout=MultiAgentRoundaboutEnv,\n",
    "    intersection=MultiAgentIntersectionEnv,\n",
    "    tollgate=MultiAgentTollgateEnv,\n",
    "    bottleneck=MultiAgentBottleneckEnv,\n",
    "    parkinglot=MultiAgentParkingLotEnv,\n",
    "    pgma=MultiAgentMetaDrive\n",
    ")\n",
    "envs = [envs_classes[CLASS_NAME]() for CLASS_NAME in envs_classes.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c940cb",
   "metadata": {},
   "source": [
    "## Real-world environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134dcda",
   "metadata": {},
   "source": [
    "We are developing new environments for benchmarking novel and challenging RL tasks! Any idea on the design of new tasks are welcomed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
