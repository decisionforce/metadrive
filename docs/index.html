<!doctype html>
<html lang="en">

<!-- === Header Starts === -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>MetaDrive</title>

    <link href="./assets/bootstrap.min.css" rel="stylesheet">
    <link href="./assets/font.css" rel="stylesheet" type="text/css">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
    <script src="./assets/jquery.min.js"></script>
    <script type="text/javascript" src="assets/corpus.js"></script>

</head>
<!-- === Header Ends === -->

<script>
    var lang_flag = 1;
</script>

<body>

<!-- === Home Section Starts === -->
<div class="section">
    <!-- === Title Starts === -->
    <div class="header">
        <div class="logo">
            <a href="https://decisionforce.github.io/" target="_blank">
                <img src="images/deciforce.png">
            </a>
        </div>
        <div class="logo"
             style="float: right; object-fit:none; overflow-x: hidden; width: 150pt; margin-top: 25pt;margin-left: 0">
            <a href="https://github.com/decisionforce/metadrive" target="_blank">
                <img style=" width: 150pt; overflow-x: hidden;"
                     align=right
                     src="https://raw.githubusercontent.com/decisionforce/metadrive/main/metadrive/assets/logo-horizon.png">
            </a>
        </div>


        <div style="padding-top: 10pt;" class="title" id="lang">
            MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning
        </div>
        <br>
    </div>
    <!-- === Title Ends === -->

    <div class="author">
        <a href="#">Quanyi Li</a><sup>1,2</sup>*,&nbsp;
        <a href="https://pengzhenghao.github.io" target="_blank">Zhenghao Peng</a><sup>1</sup>*,&nbsp;
        <a href="#">Zhenghai Xue</a><sup>1</sup>,&nbsp;
        <a href="#">Qihang Zhang</a><sup>1</sup>,&nbsp;
        <a href="http://bzhou.ie.cuhk.edu.hk" target="_blank">Bolei Zhou</a><sup>1</sup>&nbsp;
    </div>

    <div class="institution">
        <div><sup>1</sup>The Chinese University of Hong Kong,
            <sup>2</sup>Centre for Perceptual and Interactive Intelligence
        </div>
    </div>
</div>
<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
    <div class="title" id="lang">Overview of MetaDrive Simulator</div>
    <div class="body">
        <div class="teaser">
            <img src="images/panel.png">
        </div>

        <div class="text">
            To better evaluate and improve the generalization of learning-based driving systems, we introduce an
            open-ended and highly configurable driving simulator called MetaDrive.
            MetaDrive can generate a diverse set of driving scenes through both procedural content generation and
            real data import. Currently the simulator is used to study the generalization of the driving agents trained
            from reinforcement learning. See <a href="https://arxiv.org/pdf/2109.12674.pdf">paper</a> for more detail.
        </div>
    </div>
</div>
<div class="section">
    <div class="title" id="lang">Procedural Generation of Driving Scenes</div>
    <p>
    <div class="text">We first define the elementary road blocks as follows,</div>
    <div class="teaser"><img
            src="images/blocks-1row.jpg">
    </div>
    <div class="text">we then follow the proposed algorithm of procedural generation to synthesize maps:</div>

    <div align="center">
        <table width="70%" style="margin: 0 0; text-align: center;">
            <tr>
                <td>
                    <video style="display:block; width:98%; height:auto;"
                           autoplay="autoplay" controls muted loop="loop">
                        <source src="https://github.com/decisionforce/pgdrive/releases/download/pgdrive-0.1.1/BIG.mp4"
                                type="video/mp4"/>
                    </video>
                </td>
            </tr>
        </table>
    </div>


    <div class="text">We exhibit more generated maps as follows, which are further turned into interactive environments
        for reinforcement learning of end-to-end driving.
    </div>
    <div class="teaser"><img
            src="images/webpage2.001.png">
    </div>
    <div class="teaser"><img
            src="images/webpage2.002.png">
    </div>
    </p>

</div>


<div class="section">
    <div class="title" id="lang">Driving Scenes Imported from Real Data </div>
    <p>
    <div class="text">Currently, MetaDrive supports importing real traffic scenarios from Argoverse dataset,
    which enriches the diversity and reality of interactive environments for training agents.
    </div>
    <div class="teaser"><img
            src="images/real_2x4.png">
    </p>
    <p>
        <div class="text" align="left"> Compared to procedurally generated maps, Argoverse maps contain more unusual but realistic road structures,
            making it feasible to benchmark the agent performance on unseen realistic maps after training on procedurally generated maps.
        </div>
    <div class="teaser"><img
            src="images/argoverse_show_case.jpg">
    </div>
    </p>



</div>



</div>
<!-- === Overview Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
    <div class="title" id="lang">Result of Improved Generalization</div>
    <div class="body">
        <p>
        <div class="text">
            We first split the procedurally generated maps into training set and test set.
            We show that when trained with more procedurally generated maps, the driving agents from reinforcement
            learning have better generalization performance on unseen test maps, and can handle more complex scenarios.
            The detailed experimental results are in the paper. You can reproduce the experiment through <a
                href="https://github.com/decisionforce/pgdrive-generalization-paper">our generalization experiment
            code</a>.
        </div>
        <div class="teaser"><img
                src="images/sac-up.jpg">
        </div>
        <div class="teaser"><img
                src="images/sac-down.jpg">
        </div>
        </p>

        <p>
            <div class="text">
            Besides, we also benchmark the agent performance in Argoverse scenarios after training on procedurally generating maps.
            The following results demonstrate that procedural generation can help improve the agent generalization ability
            on realistic traffic scenarios.
            </div>
            <div class="teaser"><img src="images/generalization_ppo_with_real_test.png" style="width: 55%; height: 55%;"></div>
        </p>

        <p>
        <div class="text">
            The demo video of the generalizable agent is shown as follows. You can run the agent on your local machine
            through the provided example in <a href="https://github.com/decisionforce/metadrive">the simulator
            codebase</a>.
        </div>
        <div class="vedio" style="position: relative; padding-top: 50%; margin: 20pt auto; text-align: center;">
            <iframe src="https://www.youtube.com/embed/3ziJPqC_-T4" frameborder=0
                    style="position: absolute; top: 2.5%; left: 2.5%; width: 95%; height: 100%;"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
<!--        <p>-->
<!--            <a class="text" href="https://github.com/decisionforce/metadrive/releases/download/metadrive-0.1.1/agents.mp4"-->
<!--               target="_blank">-->
<!--                Download the video.-->
<!--            </a>-->
<!--        </p>-->
        </p>
    </div>
</div>
<!-- === Result Section Ends === -->


<!-- === Reference Section Starts === -->
<div class="section">
    <div class="bibtex">
        <div class="text">Citation</div>
    </div>

    If you find this work useful in your project, please consider to cite it through:

    <pre>
@misc{li2021metadrive,
      title={MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning},
      author={Quanyi Li and Zhenghao Peng and Zhenghai Xue and Qihang Zhang and Bolei Zhou},
      year={2021},
      eprint={2109.12674},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
</pre>
    <!-- Adjust the frame size based on the demo (Every project differs). -->
</div>

</body>
</html>